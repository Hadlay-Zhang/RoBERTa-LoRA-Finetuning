{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977e8058",
   "metadata": {},
   "source": [
    "# Final Submission Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac90611",
   "metadata": {},
   "source": [
    "> Authors: Yukang Luo, Zhilin Zhang, Yumeng Qian\\\n",
    "NetID: yl13427, zz10068, yq2480\\\n",
    "Team Name: LoRA Is All You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb68d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device name: NVIDIA H200\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "\n",
    "import train\n",
    "import utils\n",
    "import config\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Using device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb966ff",
   "metadata": {},
   "source": [
    "## 1. Comparison of different adaptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41dc95",
   "metadata": {},
   "source": [
    "We use the following setting to compare four different adaptors, and the summary results (best models on AG News Evaluation Set) is as follows:\n",
    "\n",
    "|    **Methods**    |    **Param Size**    |    **Target Modules**    |  **r**  |  **alpha**  |  **lr**  | **Train Epoch** | **Loss** | **Accuracy** |\n",
    "|-------------------|----------------------|--------------------------|---------|-------------|----------|---------|------------------|--------------|\n",
    "|       LoRA        |        888,580       |       query, value       |    8    |      16     |   2e-4   |    3    |    **0.2184**    |   **92.19**  |\n",
    "|      AdaLoRA      |        925,660       |    query, value, dense   |   6->4  |      2      |   2e-4   |    3    |      0.2669      |     90.78    |\n",
    "|       LoHa        |        888,580       |       query, value       |    4    |      8      |   2e-4   |    3    |      0.2536      |     91.09    |\n",
    "|       LoKr        |        632,836       |     query, key, value    |    8    |      24     |   2e-4   |    3    |      0.2536      |     91.41    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637dd904",
   "metadata": {},
   "source": [
    "### 1.1. LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ae7616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  output_dir: results_lora_qv_r8_a16_lr2e-4\n",
      "  seed: 42\n",
      "  peft_method: lora\n",
      "  target_modules: ['query', 'value']\n",
      "  lora_r: 8\n",
      "  lora_alpha: 16\n",
      "  lora_dropout: 0.1\n",
      "  learning_rate: 0.0002\n",
      "  num_train_epochs: 3\n",
      "  train_batch_size: 128\n",
      "  eval_batch_size: 128\n",
      "  optimizer: adamw_torch\n",
      "=== START TRAINING ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'results_lora_qv_r8_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed14f6577f84176b8ab297c8ac04a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aac01c4f3bc4ecfb675f8d67a53e314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ab4ff0507040828bc46e889fb9e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9378dd3bf1c46859b420069362aa576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f562593b616a490384b10d5d59e4e526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: ag_news, split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1e8637d3ff476d9187f33ffac887b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe49e0153a1498d99f267c578c7153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e225cda648894784a80b1f275da87c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359e57e2e421463b97000ab9c42cc441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26a1a8638e54c008a7a0332e4f51b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9708c4b00c7b4c969c976dec1f21a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text data:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247197d36bd44d9c97c170daeb528563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e82e7867194cd08eef8e6d6731aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=16, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.276669</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.267466</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.240532</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.231006</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.249506</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.231883</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.220692</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.229060</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.223503</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.218859</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.218379</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.217864</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9219\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =      0.246\n",
      "  train_runtime            = 0:08:39.93\n",
      "  train_samples_per_second =    688.707\n",
      "  train_steps_per_second   =      5.383\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8bde1669b8481fa3074c4a6e0ee130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.921875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.921875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9219\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_lora_qv_r8_a16_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.19%\n",
      "Best model checkpoint saved to: results_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_lora_qv_r8_a16_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_lora_qv_r8_a16_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED ===\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "args_lora = Namespace(\n",
    "    output_dir=\"results_lora_qv_r8_a16_lr2e-4\",\n",
    "    seed=42,\n",
    "    peft_method=\"lora\",\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=128,\n",
    "    optimizer=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "os.makedirs(args_lora.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k, v in vars(args_lora).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"=== START TRAINING ===\")\n",
    "final_accuracy = train.main_train(args_lora)\n",
    "print(\"=== TRAINING FINISHED ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17545963",
   "metadata": {},
   "source": [
    "### 1.2. LoHa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b634772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  output_dir: results_loha_qv_r4_a8_lr2e-4\n",
      "  seed: 42\n",
      "  peft_method: loha\n",
      "  target_modules: ['query', 'value']\n",
      "  lora_r: 4\n",
      "  lora_alpha: 8\n",
      "  rank_dropout: 0.1\n",
      "  module_dropout: 0.1\n",
      "  learning_rate: 0.0002\n",
      "  num_train_epochs: 3\n",
      "  train_batch_size: 128\n",
      "  eval_batch_size: 128\n",
      "  optimizer: adamw_torch\n",
      "=== START TRAINING ===\n",
      "Starting training process with PEFT method: loha\n",
      "Arguments:\n",
      "{'output_dir': 'results_loha_qv_r4_a8_lr2e-4', 'seed': 42, 'peft_method': 'loha', 'target_modules': ['query', 'value'], 'lora_r': 4, 'lora_alpha': 8, 'rank_dropout': 0.1, 'module_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: loha\n",
      "  Configuring LoHa with: r=4, alpha=8, rank_dropout=0.1, module_dropout=0.1\n",
      "PEFT model created with LOHA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using loha...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 09:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.751900</td>\n",
       "      <td>0.315542</td>\n",
       "      <td>0.895312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.318700</td>\n",
       "      <td>0.287417</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.274312</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.265927</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.270026</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.265025</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.265960</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.273900</td>\n",
       "      <td>0.259061</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.256121</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.256532</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.253184</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8953\n",
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9031\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9109\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_loha_qv_r4_a8_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_loha_qv_r4_a8_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =     0.3196\n",
      "  train_runtime            = 0:09:03.97\n",
      "  train_samples_per_second =    658.268\n",
      "  train_steps_per_second   =      5.145\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_loha_qv_r4_a8_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  8.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.9109375}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.9109375}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9109\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_loha_qv_r4_a8_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 91.09%\n",
      "Best model checkpoint saved to: results_loha_qv_r4_a8_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_loha_qv_r4_a8_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_loha_qv_r4_a8_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED ===\n"
     ]
    }
   ],
   "source": [
    "args_loha = Namespace(\n",
    "    output_dir=\"results_loha_qv_r4_a8_lr2e-4\",\n",
    "    seed=42,\n",
    "    peft_method=\"loha\",\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_r=4,\n",
    "    lora_alpha=8,\n",
    "    rank_dropout=0.1,\n",
    "    module_dropout=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=128,\n",
    "    optimizer=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "os.makedirs(args_loha.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k, v in vars(args_loha).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"=== START TRAINING ===\")\n",
    "final_accuracy = train.main_train(args_loha)\n",
    "print(\"=== TRAINING FINISHED ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebb932",
   "metadata": {},
   "source": [
    "### 1.3. LoKr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58364615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  output_dir: results_lokr_qkv_r8_a24_lr2e-4\n",
      "  seed: 42\n",
      "  peft_method: lokr\n",
      "  target_modules: ['query', 'key', 'value']\n",
      "  lora_r: 8\n",
      "  lora_alpha: 24\n",
      "  rank_dropout: 0.1\n",
      "  module_dropout: 0.1\n",
      "  learning_rate: 0.0002\n",
      "  num_train_epochs: 3\n",
      "  train_batch_size: 128\n",
      "  eval_batch_size: 128\n",
      "  optimizer: adamw_torch\n",
      "=== START TRAINING ===\n",
      "Starting training process with PEFT method: lokr\n",
      "Arguments:\n",
      "{'output_dir': 'results_lokr_qkv_r8_a24_lr2e-4', 'seed': 42, 'peft_method': 'lokr', 'target_modules': ['query', 'key', 'value'], 'lora_r': 8, 'lora_alpha': 24, 'rank_dropout': 0.1, 'module_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6779838114414b0e982a52ecdbffe373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning text data:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bf713839e74c768e30b6a44b75289e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lokr\n",
      "  Configuring LoKr with: r=8, alpha=24, rank_dropout=0.1, module_dropout=0.1\n",
      "PEFT model created with LOKR config.\n",
      "  Target modules: ['query', 'key', 'value']\n",
      "Trainable params: 632836 || All params: 125281544 || Trainable %: 0.51\n",
      "\n",
      "Trainable parameters (632836) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lokr...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 09:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.320631</td>\n",
       "      <td>0.892188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.293053</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.285200</td>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.269037</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.272300</td>\n",
       "      <td>0.276444</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.264061</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.260772</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.261415</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.255838</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.253571</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.253829</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8922\n",
      "Evaluation Accuracy: 0.8984\n",
      "Evaluation Accuracy: 0.8984\n",
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_lokr_qkv_r8_a24_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_lokr_qkv_r8_a24_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88394276GF\n",
      "  train_loss               =     0.2971\n",
      "  train_runtime            = 0:09:53.02\n",
      "  train_samples_per_second =    603.815\n",
      "  train_steps_per_second   =       4.72\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_lokr_qkv_r8_a24_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c1b3aa2f742af890882242caaffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.9140625}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.9140625}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9141\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_lokr_qkv_r8_a24_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 91.41%\n",
      "Best model checkpoint saved to: results_lokr_qkv_r8_a24_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_lokr_qkv_r8_a24_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_lokr_qkv_r8_a24_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED ===\n"
     ]
    }
   ],
   "source": [
    "args_lokr = Namespace(\n",
    "    output_dir=\"results_lokr_qkv_r8_a24_lr2e-4\",\n",
    "    seed=42,\n",
    "    peft_method=\"lokr\",\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_r=8,\n",
    "    lora_alpha=24,\n",
    "    rank_dropout=0.1,\n",
    "    module_dropout=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=128,\n",
    "    optimizer=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "os.makedirs(args_lokr.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k, v in vars(args_lokr).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"=== START TRAINING ===\")\n",
    "final_accuracy = train.main_train(args_lokr)\n",
    "print(\"=== TRAINING FINISHED ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812cd79",
   "metadata": {},
   "source": [
    "### 1.4. AdaLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ac5223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  output_dir: results_adalora_qvd_r4-6_a2_lr2e-4\n",
      "  seed: 42\n",
      "  peft_method: adalora\n",
      "  target_modules: ['query', 'value', 'attention.output.dense']\n",
      "  lora_r: 4\n",
      "  lora_alpha: 2\n",
      "  lora_dropout: 0.1\n",
      "  adalora_init_r: 6\n",
      "  adalora_tinit: 0\n",
      "  adalora_tfinal: 0\n",
      "  adalora_deltaT: 1\n",
      "  adalora_beta1: 0.85\n",
      "  adalora_beta2: 0.85\n",
      "  learning_rate: 0.0002\n",
      "  num_train_epochs: 3\n",
      "  train_batch_size: 128\n",
      "  eval_batch_size: 128\n",
      "  optimizer: adamw_torch\n",
      "=== START TRAINING ===\n",
      "Starting training process with PEFT method: adalora\n",
      "Arguments:\n",
      "{'output_dir': 'results_adalora_qvd_r4-6_a2_lr2e-4', 'seed': 42, 'peft_method': 'adalora', 'target_modules': ['query', 'value', 'attention.output.dense'], 'lora_r': 4, 'lora_alpha': 2, 'lora_dropout': 0.1, 'adalora_init_r': 6, 'adalora_tinit': 0, 'adalora_tfinal': 0, 'adalora_deltaT': 1, 'adalora_beta1': 0.85, 'adalora_beta2': 0.85, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: adalora\n",
      "  Configuring AdaLoRA with: target_r=4, init_r=6, dropout=0.1, tinit=0, tfinal=0, deltaT=1, beta1=0.85, beta2=0.85\n",
      "PEFT model created with ADALORA config.\n",
      "  Target modules: ['query', 'value', 'attention.output.dense']\n",
      "Trainable params: 925660 || All params: 125574404 || Trainable %: 0.74\n",
      "\n",
      "Trainable parameters (925660) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using adalora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 11:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>1.150100</td>\n",
       "      <td>0.447555</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.892188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.280307</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.273470</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.271067</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.266874</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>0.266712</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.261858</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.259384</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.260991</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.256460</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.256937</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8625\n",
      "Evaluation Accuracy: 0.8922\n",
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.8984\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9031\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_adalora_qvd_r4-6_a2_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_adalora_qvd_r4-6_a2_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88694304GF\n",
      "  train_loss               =     0.3687\n",
      "  train_runtime            = 0:11:40.33\n",
      "  train_samples_per_second =    511.295\n",
      "  train_steps_per_second   =      3.997\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_adalora_qvd_r4-6_a2_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.9078125}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.9078125}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9078\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_adalora_qvd_r4-6_a2_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 90.78%\n",
      "Best model checkpoint saved to: results_adalora_qvd_r4-6_a2_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_adalora_qvd_r4-6_a2_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_adalora_qvd_r4-6_a2_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED ===\n"
     ]
    }
   ],
   "source": [
    "args_adalora = Namespace(\n",
    "    output_dir=\"results_adalora_qvd_r4-6_a2_lr2e-4\",\n",
    "    seed=42,\n",
    "    peft_method=\"adalora\",\n",
    "    target_modules=[\"query\", \"value\", \"attention.output.dense\"],\n",
    "    lora_r=4,\n",
    "    lora_alpha=2,\n",
    "    lora_dropout=0.1,\n",
    "    adalora_init_r=6,\n",
    "    adalora_tinit=0,\n",
    "    adalora_tfinal=0,\n",
    "    adalora_deltaT=1,\n",
    "    adalora_beta1=0.85,\n",
    "    adalora_beta2=0.85,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=128,\n",
    "    optimizer=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "os.makedirs(args_adalora.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k, v in vars(args_adalora).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# launch training\n",
    "print(\"=== START TRAINING ===\")\n",
    "final_accuracy = train.main_train(args_adalora)\n",
    "print(\"=== TRAINING FINISHED ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b1c4f",
   "metadata": {},
   "source": [
    "## 2. Ablation Study of $r$ and $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511e736",
   "metadata": {},
   "source": [
    "### 2.1. Fix $r=8$, $\\alpha \\in [4, 8, 16, 32]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e72408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'output_dir': 'ablation_lora_qv_r8_a4_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 4, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=8 alpha=4 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'ablation_lora_qv_r8_a4_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 4, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=4, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.289497</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.274486</td>\n",
       "      <td>0.895312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.251587</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.238627</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.252253</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.230315</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.236980</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.231416</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.226843</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.225703</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.226529</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.8953\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9172\n",
      "Evaluation Accuracy: 0.9125\n",
      "Evaluation Accuracy: 0.9125\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9172\n",
      "Evaluation Accuracy: 0.9172\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to ablation_lora_qv_r8_a4_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to ablation_lora_qv_r8_a4_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =     0.2644\n",
      "  train_runtime            = 0:08:39.78\n",
      "  train_samples_per_second =    688.901\n",
      "  train_steps_per_second   =      5.385\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to ablation_lora_qv_r8_a4_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.921875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.921875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9219\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to ablation_lora_qv_r8_a4_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.19%\n",
      "Best model checkpoint saved to: ablation_lora_qv_r8_a4_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: ablation_lora_qv_r8_a4_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: ablation_lora_qv_r8_a4_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=8 alpha=4 ===\n",
      "Parameters: {'output_dir': 'ablation_lora_qv_r8_a8_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=8 alpha=8 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'ablation_lora_qv_r8_a8_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 8, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=8, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.286039</td>\n",
       "      <td>0.896875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.270226</td>\n",
       "      <td>0.903125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.246663</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.234369</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.248195</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.230930</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.224824</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.232998</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.227985</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.221494</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>0.221473</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>0.221513</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.8969\n",
      "Evaluation Accuracy: 0.9031\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9172\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9156\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to ablation_lora_qv_r8_a8_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to ablation_lora_qv_r8_a8_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =     0.2547\n",
      "  train_runtime            = 0:08:40.32\n",
      "  train_samples_per_second =    688.186\n",
      "  train_steps_per_second   =      5.379\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to ablation_lora_qv_r8_a8_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.921875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.921875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9219\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to ablation_lora_qv_r8_a8_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.19%\n",
      "Best model checkpoint saved to: ablation_lora_qv_r8_a8_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: ablation_lora_qv_r8_a8_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: ablation_lora_qv_r8_a8_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=8 alpha=8 ===\n",
      "Parameters: {'output_dir': 'ablation_lora_qv_r8_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=8 alpha=16 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'ablation_lora_qv_r8_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=16, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.276669</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.267466</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.240532</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.231006</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.249506</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.231883</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.220692</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.229060</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.223503</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.218859</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.218379</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.217864</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9219\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to ablation_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to ablation_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =      0.246\n",
      "  train_runtime            = 0:08:41.19\n",
      "  train_samples_per_second =    687.035\n",
      "  train_steps_per_second   =       5.37\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to ablation_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.921875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.921875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9219\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to ablation_lora_qv_r8_a16_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.19%\n",
      "Best model checkpoint saved to: ablation_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: ablation_lora_qv_r8_a16_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: ablation_lora_qv_r8_a16_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=8 alpha=16 ===\n",
      "Parameters: {'output_dir': 'ablation_lora_qv_r8_a32_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=8 alpha=32 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'ablation_lora_qv_r8_a32_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=32, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.265914</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.266870</td>\n",
       "      <td>0.907813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.238232</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.230542</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.247841</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.230885</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.218924</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.223378</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.217808</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.216078</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.214775</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>0.214816</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9078\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9125\n",
      "Evaluation Accuracy: 0.9250\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9234\n",
      "Evaluation Accuracy: 0.9219\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to ablation_lora_qv_r8_a32_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to ablation_lora_qv_r8_a32_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =     0.2366\n",
      "  train_runtime            = 0:08:40.24\n",
      "  train_samples_per_second =    688.297\n",
      "  train_steps_per_second   =       5.38\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to ablation_lora_qv_r8_a32_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.925}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.925}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.925\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to ablation_lora_qv_r8_a32_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.50%\n",
      "Best model checkpoint saved to: ablation_lora_qv_r8_a32_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: ablation_lora_qv_r8_a32_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: ablation_lora_qv_r8_a32_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=8 alpha=32 ===\n"
     ]
    }
   ],
   "source": [
    "for alpha in [4, 8, 16, 32]:\n",
    "    args = Namespace(\n",
    "        output_dir=f\"ablation_lora_qv_r8_a{alpha}_lr2e-4\",\n",
    "        seed=42,\n",
    "        peft_method=\"lora\",\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_r=8,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=0.1,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        train_batch_size=128,\n",
    "        eval_batch_size=128,\n",
    "        optimizer=\"adamw_torch\"\n",
    "    )\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    print(\"Parameters:\", vars(args))\n",
    "    print(f\"=== START TRAINING r=8 alpha={alpha} ===\")\n",
    "    final_accuracy = train.main_train(args)\n",
    "    print(f\"=== TRAINING FINISHED r=8 alpha={alpha} ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45b45e",
   "metadata": {},
   "source": [
    "### 2.2. Fix $\\alpha=16$, $r \\in [2, 4, 8]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c01394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'output_dir': 'results_lora_qv_r2_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 2, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=2 alpha=16 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'results_lora_qv_r2_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 2, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=2, alpha=16, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 667396 || All params: 125316104 || Trainable %: 0.53\n",
      "\n",
      "Trainable parameters (667396) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.426300</td>\n",
       "      <td>0.277631</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.271691</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.234512</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.227359</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.245221</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.226983</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.216865</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.224777</td>\n",
       "      <td>0.917188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.222672</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.215895</td>\n",
       "      <td>0.923438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.214486</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.213925</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9016\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9172\n",
      "Evaluation Accuracy: 0.9125\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9172\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9234\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9203\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_lora_qv_r2_a16_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_lora_qv_r2_a16_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88429682GF\n",
      "  train_loss               =     0.2473\n",
      "  train_runtime            = 0:08:45.85\n",
      "  train_samples_per_second =    680.944\n",
      "  train_steps_per_second   =      5.323\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_lora_qv_r2_a16_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.9234375}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.9234375}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9234\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_lora_qv_r2_a16_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.34%\n",
      "Best model checkpoint saved to: results_lora_qv_r2_a16_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_lora_qv_r2_a16_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_lora_qv_r2_a16_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=2 alpha=16 ===\n",
      "Parameters: {'output_dir': 'results_lora_qv_r4_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 4, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=4 alpha=16 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'results_lora_qv_r4_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 4, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=4, alpha=16, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 741124 || All params: 125389832 || Trainable %: 0.59\n",
      "\n",
      "Trainable parameters (741124) are within the limit of 1000000.\n",
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.276722</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>0.271163</td>\n",
       "      <td>0.904687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.240185</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.232284</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.249775</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.232588</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.221427</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.229906</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.225032</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.219554</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.219686</td>\n",
       "      <td>0.909375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.219382</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9047\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9125\n",
      "Evaluation Accuracy: 0.9094\n",
      "Evaluation Accuracy: 0.9125\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_lora_qv_r4_a16_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_lora_qv_r4_a16_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88505215GF\n",
      "  train_loss               =     0.2467\n",
      "  train_runtime            = 0:08:46.41\n",
      "  train_samples_per_second =    680.218\n",
      "  train_steps_per_second   =      5.317\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_lora_qv_r4_a16_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.91875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.91875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9187\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_lora_qv_r4_a16_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 91.88%\n",
      "Best model checkpoint saved to: results_lora_qv_r4_a16_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_lora_qv_r4_a16_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_lora_qv_r4_a16_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=4 alpha=16 ===\n",
      "Parameters: {'output_dir': 'results_lora_qv_r8_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "=== START TRAINING r=8 alpha=16 ===\n",
      "Starting training process with PEFT method: lora\n",
      "Arguments:\n",
      "{'output_dir': 'results_lora_qv_r8_a16_lr2e-4', 'seed': 42, 'peft_method': 'lora', 'target_modules': ['query', 'value'], 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'learning_rate': 0.0002, 'num_train_epochs': 3, 'train_batch_size': 128, 'eval_batch_size': 128, 'optimizer': 'adamw_torch'}\n",
      "Using device: cuda\n",
      "Set seed to 42\n",
      "Loading tokenizer for model: roberta-base\n",
      "Loading dataset: ag_news, split: train\n",
      "Text cleaning completed for text column.\n",
      "Number of labels: 4\n",
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 119360\n",
      "Eval dataset size: 640\n",
      "Calculated total training steps: 2799\n",
      "Loading base model: roberta-base for 4 labels.\n",
      "Creating PEFT model using method: lora\n",
      "  Configuring LoRA with: r=8, alpha=16, dropout=0.1\n",
      "PEFT model created with LORA config.\n",
      "  Target modules: ['query', 'value']\n",
      "Trainable params: 888580 || All params: 125537288 || Trainable %: 0.71\n",
      "\n",
      "Trainable parameters (888580) are within the limit of 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/RoBERTa-LoRA-Finetuning/train.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PEFT model training using lora...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799' max='2799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2799/2799 08:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.276669</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.267466</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.240532</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.231006</td>\n",
       "      <td>0.915625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.249506</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.231883</td>\n",
       "      <td>0.910937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.220692</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1864</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.229060</td>\n",
       "      <td>0.914062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.223503</td>\n",
       "      <td>0.920312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.218859</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2563</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.218379</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2796</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.217864</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9000\n",
      "Evaluation Accuracy: 0.9062\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9156\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9109\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9141\n",
      "Evaluation Accuracy: 0.9203\n",
      "Evaluation Accuracy: 0.9187\n",
      "Evaluation Accuracy: 0.9219\n",
      "Evaluation Accuracy: 0.9219\n",
      "\n",
      "Callback: Saving final model checkpoint (end of training) to results_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "Callback: Final model checkpoint saved successfully to results_lora_qv_r8_a16_lr2e-4/last_checkpoint\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 88656280GF\n",
      "  train_loss               =      0.246\n",
      "  train_runtime            = 0:08:40.86\n",
      "  train_samples_per_second =    687.467\n",
      "  train_steps_per_second   =      5.374\n",
      "\n",
      "Saving BEST model checkpoint identified by Trainer to results_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "\n",
      "Evaluating the final best model on the evaluation set...\n",
      "Starting Evaluating on 640 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metric (accuracy): {'accuracy': 0.921875}\n",
      "Final Evaluation Metrics (Best Model): {'accuracy': 0.921875}\n",
      "***** eval_final_best metrics *****\n",
      "  accuracy = 0.9219\n",
      "\n",
      "Generating curves plot from callback logs...\n",
      "Plotting metrics from 13 points in metrics_log.jsonl...\n",
      "Dual-axis plot saved to results_lora_qv_r8_a16_lr2e-4/training_curves_dual_axis.png\n",
      "Plot generation complete.\n",
      "\n",
      "==================================================\n",
      "TRAINING COMPLETED\n",
      "Best model validation accuracy (evaluated at end): 92.19%\n",
      "Best model checkpoint saved to: results_lora_qv_r8_a16_lr2e-4/best_checkpoint\n",
      "Fractional epoch metrics logged to: results_lora_qv_r8_a16_lr2e-4/metrics_log.jsonl\n",
      "Training curves plot saved to: results_lora_qv_r8_a16_lr2e-4/training_curves.png\n",
      "==================================================\n",
      "\n",
      "=== TRAINING FINISHED r=8 alpha=16 ===\n"
     ]
    }
   ],
   "source": [
    "for r in [2, 4, 8]:\n",
    "    args = Namespace(\n",
    "        output_dir=f\"results_lora_qv_r{r}_a16_lr2e-4\",\n",
    "        seed=42,\n",
    "        peft_method=\"lora\",\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_r=r,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        train_batch_size=128,\n",
    "        eval_batch_size=128,\n",
    "        optimizer=\"adamw_torch\"\n",
    "    )\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    print(\"Parameters:\", vars(args))\n",
    "    print(f\"=== START TRAINING r={r} alpha=16 ===\")\n",
    "    final_accuracy = train.main_train(args)\n",
    "    print(f\"=== TRAINING FINISHED r={r} alpha=16 ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
